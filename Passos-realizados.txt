1. Criar uma conta no Google Cloud.

2. Criar um projeto chamado "e-commerce-data".
3. Criar um tópico chamado "MyTopic".
4. Criar assinaturas chamadas "MySub" e "MyTopic-sub". Mas ainda não especifiquei o que cada uma consumirá.

5. Criar uma instância dentro do BigTable para armazenar esses dados.
6. Instância criada de 3GB de 1 nó, dentro da zona de Berlim. Valor estimado: 3 dólares mensais.
Essa instância está em desenvolvimento pois em produção é cobrado por hora.

7. Escolher a base de dados que serão usados e armazenados no GCP : https://docs.cloud.google.com/docs/authentication/set-up-adc-local-dev-environment?hl=pt-br
8. Instalar localmente a CLI do Google Cloud : https://docs.cloud.google.com/docs/authentication/set-up-adc-local-dev-environment?hl=pt-br
9. Criar script para simulação de ingestão de dados em tempo real.
10. Dados necessários para script conectar ao GCP: ID do projeto e ID do tópico.

------------------------------------------------------------------------------------------
Obs:
Só consegui rodar o script no console CLI, não consegui rodar nem colocando como variável de ambiente.
Não publiquei ainda efetivamente nenhuma mensagem no tópico para não consumir recursos.
------------------------------------------------------------------------------------------
Perguntas:
Após publicar no tópico, como conectar no Dataflow / Apache Beam ?
Criar job no site do Google Cloud e script para ETL ?
Dá para automatizar usando terraform (por causa do tempo)?
